{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30965abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sys\n",
    "import pandas\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import time\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "# from keras.layers import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import (LSTM, BatchNormalization, Dense, Dropout, Embedding,Input, Lambda, TimeDistributed)\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from npy_append_array import NpyAppendArray\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "179934b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = 'Flickr_Data/Images/'\n",
    "images = glob(images_path+'*.jpg')\n",
    "captions_path = \"Flickr_Data/Flickr_TextData/FLickr8k.token.txt\"\n",
    "# captions_path = \"results.csv\"\n",
    "max_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552d80e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sets train, test and dev\n",
    "\n",
    "train = set()\n",
    "f = open(\"Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt\", \"r\")\n",
    "for line in f:\n",
    "    train.add(line.strip())\n",
    "f.close()\n",
    "\n",
    "test = set()\n",
    "f = open(\"Flickr_Data/Flickr_TextData/Flickr_8k.testImages.txt\", \"r\")\n",
    "for line in f:\n",
    "    test.add(line.strip())\n",
    "f.close()\n",
    "\n",
    "dev = set()\n",
    "f = open(\"Flickr_Data/Flickr_TextData/Flickr_8k.devImages.txt\", \"r\")\n",
    "for line in f:\n",
    "    dev.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f12d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the images\n",
    "# generate a dictionary of image filename -> feature vector\n",
    "# start_index is inclusive\n",
    "\n",
    "def generate_feature_vectors(start_index, num_of_images, images, model):\n",
    "    img_feature_vectors = {}\n",
    "\n",
    "    count = 0\n",
    "    for item in images[start_index:]:\n",
    "        img = cv2.imread(item)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = img.reshape(1, 224, 224, 3)\n",
    "\n",
    "        feature_vector = model.predict(img, verbose=0).reshape(2048,)\n",
    "\n",
    "        img_filename = item.split('\\\\')[-1]\n",
    "        img_feature_vectors[img_filename] = feature_vector\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if (count%50==0):\n",
    "            print(count)\n",
    "\n",
    "        if (count==num_of_images):\n",
    "            break\n",
    "\n",
    "    return img_feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77452d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes a string lowercase, prepends it with the string 'sos' and appends with 'eos'\n",
    "def process_string(s):\n",
    "    s = s.lower()\n",
    "    s = 'sos ' + s + ' eos'\n",
    "#     s = s.replace(\" a \", \" \")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the captions data\n",
    "\n",
    "# generate a dictionary of image filename -> list of captions\n",
    "\n",
    "def generate_captions_dict(captions_path, img_feature_vectors):\n",
    "    captions_dict = {}\n",
    "\n",
    "    f = open(captions_path, 'r').read().split('\\n')\n",
    "\n",
    "    # generate a dictionary of filenames to a list of captions\n",
    "    for line in f:\n",
    "        try:\n",
    "            filename_caption = line.split('\\t') \n",
    "            filename = filename_caption[0][:-2]\n",
    "            caption = process_string(filename_caption[1])\n",
    "\n",
    "            if filename in img_feature_vectors:\n",
    "                if filename not in captions_dict:\n",
    "                    captions_dict[filename] = [caption]\n",
    "                else:\n",
    "                    captions_dict[filename].append(caption)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return captions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7380afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for flickr30k\n",
    "\n",
    "def generate_captions_dict(captions_path, img_feature_vectors):\n",
    "    captions_dict = {}\n",
    "    \n",
    "    f = open(captions_path, 'r', encoding='utf8')\n",
    "    for line in f:\n",
    "        try:\n",
    "            filename_caption = line.split(\"|\")\n",
    "            filename = filename_caption[0]\n",
    "            caption = process_string(filename_caption[2].strip())\n",
    "            \n",
    "            if filename in img_feature_vectors:\n",
    "                if filename not in captions_dict:\n",
    "                    captions_dict[filename] = [caption]\n",
    "                else:\n",
    "                    captions_dict[filename].append(caption)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    f.close()\n",
    "    return captions_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a list of all captions in a text file \n",
    "\n",
    "# def get_list_of_captions(captions_path):\n",
    "#     f = open(captions_path, 'r').read().split('\\n')\n",
    "#     all_captions = []\n",
    "\n",
    "#     for line in f:\n",
    "#         try:\n",
    "#             filename_caption = line.split('\\t') \n",
    "#             caption = process_string(filename_caption[1])\n",
    "#             all_captions.append(caption)\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#     return all_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dictionary called vocab which will contain all words in the list of captions mapped to a unique integer\n",
    "# does not include words that appear less than n times\n",
    "# only keeps common words\n",
    "\n",
    "# def create_common_vocab(captions_path, n):\n",
    "#     vocab_freq = {}\n",
    "#     vocab_dict = {}\n",
    "#     all_captions = get_list_of_captions(captions_path)\n",
    "#     count = 1\n",
    "#     for caption in all_captions:\n",
    "#         caption_as_list = caption.split()\n",
    "#         for word in caption_as_list:\n",
    "#             if word not in vocab_freq:\n",
    "#                 vocab_freq[word] = 1\n",
    "#             else:\n",
    "#                 vocab_freq[word] = vocab_freq[word]+1\n",
    "#     vocab_list = [w for w in vocab_freq if vocab_freq[w] >= n]\n",
    "#     count = 1\n",
    "#     for word in vocab_list:\n",
    "#         if word not in vocab_dict:\n",
    "#             vocab_dict[word] = count\n",
    "#             count+=1\n",
    "#     return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_common_vocab(captions_dict, n):\n",
    "    vocab_freq = {}\n",
    "    vocab_dict = {}\n",
    "    for key, value in captions_dict.items():\n",
    "        for caption in value:\n",
    "            caption_as_list = caption.split()\n",
    "            for word in caption_as_list:\n",
    "                if word not in vocab_freq:\n",
    "                    vocab_freq[word] = 1\n",
    "                else:\n",
    "                    vocab_freq[word] = vocab_freq[word]+1\n",
    "    vocab_list = [w for w in vocab_freq if vocab_freq[w] >= n]\n",
    "    count = 1\n",
    "    for word in vocab_list:\n",
    "        if word not in vocab_dict:\n",
    "            vocab_dict[word] = count\n",
    "            count+=1\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ff4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a string and returns a list of integers where each integer corresponds to a particular word\n",
    "def encode_string(s, vocab):\n",
    "    s_list = s.split()\n",
    "    encoded_string = []\n",
    "    for word in s_list:\n",
    "            if word in vocab:\n",
    "                encoded_string.append(vocab[word])\n",
    "    return encoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61624e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a copy of captions_dict where each caption is replaced with a list of integers where each integer corresponds to a word in the caption\n",
    "\n",
    "def encode_captions_dict(captions_dict, vocab):\n",
    "    captions_dict_encoded = copy.deepcopy(captions_dict)\n",
    "\n",
    "    for filename, captions in captions_dict_encoded.items():\n",
    "        for i, caption in enumerate(captions):\n",
    "            captions[i] = encode_string(caption, vocab)\n",
    "    return captions_dict_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7222b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get length of the longest caption in the data set\n",
    "def get_max_length(captions_dict):\n",
    "    max_len = 0\n",
    "    for key, value in captions_dict.items():\n",
    "        for caption in value:\n",
    "            if len(caption.split())>max_len:\n",
    "                max_len = len(caption.split())\n",
    "    return max_len\n",
    "\n",
    "# max_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c7689a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size -> number of images will data be generated for at once\n",
    "\n",
    "def generate_training_data(image_feature_vectors, captions_dict_encoded, max_len, vocab_len, batch_size, num_of_images):\n",
    "    X_file = 'X_dev.npy'\n",
    "    y_in_file = 'y_in_dev.npy'\n",
    "    y_out_file = 'y_out_dev.npy'\n",
    "    filenames_file = 'filenames_dev.npy'\n",
    "    \n",
    "    X = []\n",
    "    y_in = []\n",
    "    y_out = []\n",
    "    filenames = set()\n",
    "    \n",
    "    n = 0\n",
    "    for filename, captions in captions_dict_encoded.items():\n",
    "        n+=1\n",
    "#         if n <= 6473:\n",
    "#             continue\n",
    "        for caption in captions:\n",
    "            i = 0\n",
    "            for word in caption:\n",
    "                if i==0:\n",
    "                    i+=1\n",
    "                    continue\n",
    "                y_in_item = [caption[:i]]\n",
    "                y_in_item = pad_sequences(y_in_item, maxlen=max_len, truncating='post')[0]\n",
    "                y_in.append(y_in_item)\n",
    "\n",
    "                y_out_item = to_categorical([word], num_classes=vocab_len+1)[0]\n",
    "                y_out.append(y_out_item)\n",
    "\n",
    "                X_item = image_feature_vectors[filename]\n",
    "                X.append(X_item)\n",
    "                \n",
    "                filenames.add(filename)\n",
    "                i+=1\n",
    "        if n%batch_size==0 or n==num_of_images:\n",
    "            print(n)\n",
    "            # shuffle rows before saving\n",
    "            with NpyAppendArray(X_file) as npaa:\n",
    "                npaa.append(np.array(X))\n",
    "            with NpyAppendArray(y_in_file) as npaa:\n",
    "                npaa.append(np.array(y_in))\n",
    "            with NpyAppendArray(y_out_file) as npaa:\n",
    "                npaa.append(np.array(y_out))\n",
    "            X = []\n",
    "            y_in = []\n",
    "            y_out = []\n",
    "            if n == num_of_images:\n",
    "                break\n",
    "    np.save(filenames_file, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResNet50(include_top=True)\n",
    "# restructure model\n",
    "resnet_model = Model(inputs=resnet_model.input, outputs=resnet_model.layers[-2].output)\n",
    "\n",
    "img_feature_vectors = generate_feature_vectors(0, 8091, images, resnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b98763",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict = generate_captions_dict(captions_path, img_feature_vectors)\n",
    "vocab = create_common_vocab(captions_dict, 10)\n",
    "captions_dict_encoded = encode_captions_dict(captions_dict, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0888e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('img_feature_vectors.npy', img_feature_vectors)\n",
    "# np.save('captions_dict.npy', captions_dict)\n",
    "# np.save('captions_dict_encoded.npy', captions_dict_encoded)\n",
    "# np.save('vocab.npy', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d486c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_feature_vectors = np.load('img_feature_vectors.npy', allow_pickle='TRUE').item()\n",
    "# captions_dict_encoded = np.load('captions_dict_encoded.npy', allow_pickle='TRUE').item()\n",
    "vocab = np.load(\"vocab.npy\", allow_pickle='TRUE').item()\n",
    "# embedding_matrix = np.load('embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c883a879",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21880/469712531.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcaptions_dict_encoded_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcaptions_dict_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdev\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dev' is not defined"
     ]
    }
   ],
   "source": [
    "captions_dict_encoded_dev = {key: captions_dict_encoded[key] for key in dev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "325a0e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "5.8042988777160645\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "generate_training_data(img_feature_vectors, captions_dict_encoded_dev, 40, vocab_len, 1000, 1000)\n",
    "\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5022a3",
   "metadata": {},
   "source": [
    "Word Embeddings and other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a91dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "f = open(\"glove/glove.6B.200d.txt\", encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "    vector = np.asarray(word_vector[1:], dtype='float32')\n",
    "    word_embeddings[word] = vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a3800e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.load(\"vocab.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10cb9866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.013748 , -0.28352  , -0.33465  ,  0.29837  ,  0.61233  ,\n",
       "        0.60097  ,  0.4735   ,  0.42694  ,  0.32204  , -0.22364  ,\n",
       "       -0.27225  ,  0.085531 , -0.1714   ,  0.48952  , -0.062202 ,\n",
       "       -0.66706  , -0.54124  ,  0.37751  ,  0.042753 , -0.0039544,\n",
       "        0.483    , -0.45625  ,  0.35832  , -0.1135   , -0.013117 ,\n",
       "       -1.1562   ,  0.092714 ,  0.1407   , -0.22857  ,  0.087142 ,\n",
       "        0.31291  ,  0.11429  ,  0.1032   ,  0.18801  ,  0.21747  ,\n",
       "       -0.010851 ,  0.095892 , -0.40989  , -0.17735  , -0.33253  ,\n",
       "        0.08932  , -0.019247 , -0.039571 ,  0.12407  ,  0.69643  ,\n",
       "       -0.44594  ,  0.42551  , -0.019997 , -0.058083 ,  0.14584  ,\n",
       "        0.12179  , -0.18677  ,  0.44433  ,  0.21295  ,  0.001713 ,\n",
       "        0.15448  , -0.82974  , -0.29175  , -0.038278 , -0.68382  ,\n",
       "       -0.62029  , -0.30044  , -0.11898  ,  0.15675  , -0.49229  ,\n",
       "        0.37115  , -0.28078  , -0.37748  ,  0.34853  ,  0.087878 ,\n",
       "        0.37517  , -0.61631  ,  0.35614  , -0.53103  , -0.043028 ,\n",
       "       -0.40643  , -0.12396  , -0.066833 ,  0.31317  , -0.4281   ,\n",
       "        0.56852  ,  0.28694  ,  0.63509  ,  0.38842  ,  0.43338  ,\n",
       "        0.021186 , -0.13535  , -0.44132  ,  0.68092  , -0.27765  ,\n",
       "        0.13902  ,  0.04955  , -0.59734  ,  0.25539  ,  0.4115   ,\n",
       "        0.20381  , -0.033061 , -0.2497   , -0.27421  ,  0.31622  ,\n",
       "       -0.24433  ,  0.40109  ,  0.12754  , -0.021723 , -0.33716  ,\n",
       "        0.45559  , -0.39903  , -0.033763 ,  0.62037  ,  0.10298  ,\n",
       "       -0.41464  , -0.68771  , -0.38188  , -0.55306  , -0.55716  ,\n",
       "        0.31674  ,  0.25217  , -0.45359  , -0.29328  , -0.59262  ,\n",
       "       -0.28879  ,  0.089908 , -0.63507  , -0.21521  ,  0.39386  ,\n",
       "        0.38624  ,  0.8563   , -0.058761 , -0.60798  ,  0.19389  ,\n",
       "        0.14078  , -0.10343  ,  0.61297  , -0.29483  , -0.50789  ,\n",
       "       -0.49199  , -0.049991 ,  0.04229  , -0.30115  ,  1.0892   ,\n",
       "        0.31394  , -0.17763  ,  0.76742  , -0.63703  ,  0.16276  ,\n",
       "        0.24877  ,  0.023458 , -0.032319 , -0.38451  ,  0.12863  ,\n",
       "        0.21927  ,  0.30029  ,  0.38282  , -0.22482  , -0.35998  ,\n",
       "        0.50628  , -0.36227  , -0.088798 , -0.043762 ,  0.42279  ,\n",
       "        0.32564  ,  0.2989   , -0.25508  , -0.045344 , -0.31353  ,\n",
       "        0.077535 ,  0.10833  ,  0.062986 , -0.036769 ,  0.12351  ,\n",
       "        0.664    , -0.22658  ,  0.32541  ,  0.76789  ,  0.30794  ,\n",
       "        0.078603 , -0.79168  ,  0.98599  , -0.59347  ,  0.33321  ,\n",
       "        0.4237   ,  0.11723  ,  0.25621  , -0.51116  , -0.17306  ,\n",
       "       -0.069517 , -0.16701  , -0.10174  ,  0.13958  , -0.21497  ,\n",
       "        0.39712  ,  0.13427  ,  0.60592  , -0.44455  ,  0.098218 ,\n",
       "       -0.16489  ,  0.018061 , -0.75374  , -0.26982  , -0.3311   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.get(\"t-shirt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6256204",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "vocab_size = 1965+1\n",
    "# word to index\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "\n",
    "for word, i in vocab.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = word_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a89633aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embedding_matrix.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d13ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 1\n",
    "\n",
    "embedding_size = 128\n",
    "vocab_len = 3988+1\n",
    "max_len = 40\n",
    "\n",
    "#image\n",
    "x = keras.Sequential([\n",
    "    keras.layers.Input(shape=(2048,)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.RepeatVector(max_len)\n",
    "])\n",
    "\n",
    "#caption\n",
    "y = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_len, embedding_size, input_length=max_len),\n",
    "    keras.layers.LSTM(256, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(embedding_size))\n",
    "])\n",
    "\n",
    "#concatinate outputs of y and x\n",
    "z = keras.layers.Concatenate()([x.output, y.output])\n",
    "z = keras.layers.LSTM(128, return_sequences=True)(z)\n",
    "z = keras.layers.LSTM(512, return_sequences=False)(z)\n",
    "z = keras.layers.Dense(vocab_len, activation='softmax')(z)\n",
    "\n",
    "model = Model(inputs=[x.input, y.input], outputs=z)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea2a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 2\n",
    "\n",
    "vocab_size = 1965+1\n",
    "max_len = 40\n",
    "embedding_size = 200\n",
    "\n",
    "# image feature extractor model\n",
    "image_input = Input(shape=(2048,))\n",
    "ii1 = Dropout(0.5)(image_input)\n",
    "ii2 = Dense(256, activation='relu')(ii1)\n",
    "\n",
    "# partial caption sequence model\n",
    "caption_input = Input(shape=(max_len,))\n",
    "ci1 = Embedding(vocab_size, embedding_size, mask_zero=True)(caption_input)\n",
    "ci2 = Dropout(0.5)(ci1)\n",
    "ci3 = LSTM(256)(ci2)\n",
    "\n",
    "# decoder (feed forward) model\n",
    "output = add([ii2, ci3])\n",
    "output = Dense(256, activation='relu')(output)\n",
    "output = Dense(vocab_size, activation='softmax')(output)\n",
    "\n",
    "# merge the two input models\n",
    "model = Model(inputs=[image_input, caption_input ], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dcdbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
