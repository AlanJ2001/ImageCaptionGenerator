{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30965abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sys\n",
    "import pandas\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import time\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "# from keras.layers import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import (LSTM, BatchNormalization, Dense, Dropout, Embedding,Input, Lambda, TimeDistributed)\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from npy_append_array import NpyAppendArray\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "179934b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_path = 'Flickr_Data/Images/'\n",
    "# images = glob(images_path+'*.jpg')\n",
    "# captions_path = \"results.csv\"\n",
    "captions_path = \"results.csv\"\n",
    "# captions_path = \"results.csv\"\n",
    "max_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552d80e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sets train, test and dev\n",
    "\n",
    "train = set()\n",
    "f = open(\"Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt\", \"r\")\n",
    "for line in f:\n",
    "    train.add(line.strip())\n",
    "f.close()\n",
    "\n",
    "test = set()\n",
    "f = open(\"Flickr_Data/Flickr_TextData/Flickr_8k.testImages.txt\", \"r\")\n",
    "for line in f:\n",
    "    test.add(line.strip())\n",
    "f.close()\n",
    "\n",
    "dev = set()\n",
    "f = open(\"Flickr_Data/Flickr_TextData/Flickr_8k.devImages.txt\", \"r\")\n",
    "for line in f:\n",
    "    dev.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27e73d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet():\n",
    "    resnet_model = ResNet50(include_top=True)\n",
    "    resnet_model = Model(inputs=resnet_model.input, outputs=resnet_model.layers[-2].output)\n",
    "    return resnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f12d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the images\n",
    "# generate a dictionary of image filename -> feature vector\n",
    "# start_index is inclusive\n",
    "\n",
    "def generate_feature_vectors(num_of_images, images_path, model):\n",
    "    img_feature_vectors = {}\n",
    "    images = [f for f in os.listdir(images_path) if os.path.isfile(os.path.join(images_path, f)) and f.endswith('.jpg')]\n",
    "    \n",
    "    count = 0\n",
    "    for item in images[:num_of_images]:\n",
    "        img = cv2.imread(images_path+item)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = img.reshape(1, 224, 224, 3)\n",
    "\n",
    "        feature_vector = model.predict(img, verbose=0).reshape(2048,)\n",
    "\n",
    "        img_feature_vectors[item] = feature_vector\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if (count%50==0):\n",
    "            print(count)\n",
    "\n",
    "    return img_feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e77452d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes a string lowercase, prepends it with the string 'sos' and appends with 'eos'\n",
    "def process_string(s):\n",
    "    s = s.lower()\n",
    "    s = 'sos ' + s + ' eos'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79c66cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_captions_dict(captions_path, img_feature_vectors):\n",
    "    df = pd.read_csv(captions_path, delimiter='|')\n",
    "    df.columns = ['image_name', 'comment_number', 'comment']\n",
    "    del df['comment_number']\n",
    "    df['comment'][19999] = ' A dog runs across the grass .'\n",
    "    captions_dict = {}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        filename = row['image_name']\n",
    "        caption = process_string(row['comment'])\n",
    "        if filename in img_feature_vectors:\n",
    "            if filename not in captions_dict:\n",
    "                captions_dict[filename] = [caption]\n",
    "            else:\n",
    "                captions_dict[filename].append(caption)\n",
    "    return captions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f499ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove\n",
    "\n",
    "def generate_captions_dict(captions_path, img_feature_vectors):\n",
    "    captions_dict = {}\n",
    "\n",
    "    f = open(captions_path, 'r').read().split('\\n')\n",
    "\n",
    "    # generate a dictionary of filenames to a list of captions\n",
    "    for line in f:\n",
    "        try:\n",
    "            filename_caption = line.split('\\t') \n",
    "            filename = filename_caption[0][:-2]\n",
    "            caption = process_string(filename_caption[1])\n",
    "\n",
    "            if filename in img_feature_vectors:\n",
    "                if filename not in captions_dict:\n",
    "                    captions_dict[filename] = [caption]\n",
    "                else:\n",
    "                    captions_dict[filename].append(caption)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return captions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8d7bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(captions_dict, n, train):\n",
    "    vocab_freq = {}\n",
    "    vocab_dict = {}\n",
    "    for key, value in captions_dict.items():\n",
    "        if key not in train:\n",
    "            continue\n",
    "        for caption in value:\n",
    "            caption_as_list = caption.split()\n",
    "            for word in caption_as_list:\n",
    "                if word not in vocab_freq:\n",
    "                    vocab_freq[word] = 1\n",
    "                else:\n",
    "                    vocab_freq[word] = vocab_freq[word]+1\n",
    "    words_to_keep = [w for w in vocab_freq if vocab_freq[w] >= n]\n",
    "    \n",
    "    count = 1\n",
    "    for word in words_to_keep:\n",
    "        if word not in vocab_dict:\n",
    "            vocab_dict[word] = count\n",
    "            count+=1\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b7ff4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_string(s, vocab):\n",
    "    s_list = s.split()\n",
    "    encoded_string = []\n",
    "    for word in s_list:\n",
    "        if word in vocab:\n",
    "            encoded_string.append(vocab[word])\n",
    "    return encoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61624e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_captions_dict(captions_dict, vocab):\n",
    "    captions_dict_encoded = copy.deepcopy(captions_dict)\n",
    "\n",
    "    for filename, captions in captions_dict_encoded.items():\n",
    "        for i, caption in enumerate(captions):\n",
    "            captions[i] = encode_string(caption, vocab)\n",
    "    return captions_dict_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c7689a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size -> number of images will data be generated for at once\n",
    "\n",
    "#hello\n",
    "def generate_training_data(img_feature_vectors, captions_dict_encoded, max_len, vocab_len, batch_size, id_):\n",
    "    \n",
    "    X_file = f'X_{id_}.npy'\n",
    "    y_in_file = f'y_in_{id_}.npy'\n",
    "    y_out_file = f'y_out_{id_}.npy'\n",
    "    filenames_file = f'filenames_{id_}.npy'\n",
    "    \n",
    "    X = []\n",
    "    y_in = []\n",
    "    y_out = []\n",
    "    filenames = set()\n",
    "    \n",
    "    n = 0\n",
    "    for filename, fv in img_feature_vectors.items():\n",
    "        n+=1\n",
    "        for caption in captions_dict_encoded[filename]:\n",
    "            i = 0\n",
    "            for word in caption:\n",
    "                if i==0:\n",
    "                    i+=1\n",
    "                    continue\n",
    "                y_in_item = [caption[:i]]\n",
    "                y_in_item = pad_sequences(y_in_item, maxlen=max_len, truncating='post')[0]\n",
    "                y_in.append(y_in_item)\n",
    "\n",
    "                y_out_item = to_categorical([word], num_classes=vocab_len+1)[0]\n",
    "                y_out.append(y_out_item)\n",
    "\n",
    "                X.append(fv)\n",
    "                \n",
    "                filenames.add(filename)\n",
    "                i+=1\n",
    "        if n%batch_size==0 or n==len(img_feature_vectors):\n",
    "            print(n)\n",
    "            with NpyAppendArray(X_file) as npaa:\n",
    "                npaa.append(np.array(X))\n",
    "            with NpyAppendArray(y_in_file) as npaa:\n",
    "                npaa.append(np.array(y_in))\n",
    "            with NpyAppendArray(y_out_file) as npaa:\n",
    "                npaa.append(np.array(y_out))\n",
    "            X = []\n",
    "            y_in = []\n",
    "            y_out = []\n",
    "            if n == len(img_feature_vectors):\n",
    "                break\n",
    "    np.save(filenames_file, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8b98763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet_model = resnet()\n",
    "# img_feature_vectors = generate_feature_vectors(8091, images_path, resnet_model)\n",
    "captions_dict = generate_captions_dict(captions_path, img_feature_vectors)\n",
    "vocab = create_vocab(captions_dict, 10, train)\n",
    "captions_dict_encoded = encode_captions_dict(captions_dict, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [f for f in os.listdir(images_path) if os.path.isfile(os.path.join(images_path, f)) and f.endswith('.jpg')]\n",
    "images = set(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0798de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_filenames(filenames):\n",
    "    modified_filenames = set()\n",
    "    for filename in filenames:\n",
    "        parts = filename.split(\"_\")\n",
    "        modified_filename = \"\".join(parts[:-1]) + \".\" + parts[-1].split(\".\")[-1]\n",
    "        modified_filenames.add(modified_filename)\n",
    "    return modified_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = modify_filenames(train)\n",
    "test = modify_filenames(test)\n",
    "dev = modify_filenames(dev)\n",
    "images = modify_filenames(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0888e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('30k_final/img_feature_vectors.npy', img_feature_vectors)\n",
    "# np.save('30k_final/captions_dict.npy', captions_dict)\n",
    "# np.save('30k_final/captions_dict_encoded.npy', captions_dict_encoded)\n",
    "# np.save('30k_final/vocab_new.npy', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d486c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feature_vectors = np.load('30k/img_feature_vectors.npy', allow_pickle='TRUE').item()\n",
    "# captions_dict_encoded = np.load('captions_dict_encoded.npy', allow_pickle='TRUE').item()\n",
    "# vocab = np.load(\"vocab.npy\", allow_pickle='TRUE').item()\n",
    "# embedding_matrix = np.load('embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daab4835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31783"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "835afbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list(img_feature_vectors.keys())\n",
    "train = set(images[:25427])\n",
    "dev = set(images[25427:28605])\n",
    "test = set(images[28605:31783])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a86941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feature_vectors_new = {k: v for k, v in img_feature_vectors.items() if k in dev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "325a0e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "3178\n",
      "62.444525957107544\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "generate_training_data(img_feature_vectors_new, captions_dict_encoded, 40, vocab_len, 1000, \"dev\")\n",
    "\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5022a3",
   "metadata": {},
   "source": [
    "Word Embeddings and other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "f = open(\"glove/glove.6B.200d.txt\", encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "    vector = np.asarray(word_vector[1:], dtype='float32')\n",
    "    word_embeddings[word] = vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3800e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.load(\"vocab.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings.get(\"t-shirt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6256204",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "vocab_size = 1965+1\n",
    "# word to index\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "\n",
    "for word, i in vocab.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = word_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89633aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embedding_matrix.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d13ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 1\n",
    "\n",
    "embedding_size = 128\n",
    "vocab_len = 3988+1\n",
    "max_len = 40\n",
    "\n",
    "#image\n",
    "x = keras.Sequential([\n",
    "    keras.layers.Input(shape=(2048,)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.RepeatVector(max_len)\n",
    "])\n",
    "\n",
    "#caption\n",
    "y = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_len, embedding_size, input_length=max_len),\n",
    "    keras.layers.LSTM(256, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(embedding_size))\n",
    "])\n",
    "\n",
    "#concatinate outputs of y and x\n",
    "z = keras.layers.Concatenate()([x.output, y.output])\n",
    "z = keras.layers.LSTM(128, return_sequences=True)(z)\n",
    "z = keras.layers.LSTM(512, return_sequences=False)(z)\n",
    "z = keras.layers.Dense(vocab_len, activation='softmax')(z)\n",
    "\n",
    "model = Model(inputs=[x.input, y.input], outputs=z)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea2a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 2\n",
    "\n",
    "vocab_size = 1965+1\n",
    "max_len = 40\n",
    "embedding_size = 200\n",
    "\n",
    "# image feature extractor model\n",
    "image_input = Input(shape=(2048,))\n",
    "ii1 = Dropout(0.5)(image_input)\n",
    "ii2 = Dense(256, activation='relu')(ii1)\n",
    "\n",
    "# partial caption sequence model\n",
    "caption_input = Input(shape=(max_len,))\n",
    "ci1 = Embedding(vocab_size, embedding_size, mask_zero=True)(caption_input)\n",
    "ci2 = Dropout(0.5)(ci1)\n",
    "ci3 = LSTM(256)(ci2)\n",
    "\n",
    "# decoder (feed forward) model\n",
    "output = add([ii2, ci3])\n",
    "output = Dense(256, activation='relu')(output)\n",
    "output = Dense(vocab_size, activation='softmax')(output)\n",
    "\n",
    "# merge the two input models\n",
    "model = Model(inputs=[image_input, caption_input ], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c7952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "s = \"string. With. hello-there Punctua%tion? hello' \"\n",
    "s = re.sub(r'[^\\w\\s\\'\\-]','',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "alist = [\"a dog on a water slide\", \"a boy jumping at a tree\", \"a man sailing a boat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d00bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1+1,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(alist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b4bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef744c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_counts['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b558a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer which contains all words\n",
    "# when encoding only use word if its frequency is greater than 10 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
