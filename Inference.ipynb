{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77bbb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications import ResNet50\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "import time\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from math import log, exp\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbe44f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "import sys\n",
    "import pandas\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "# from keras.layers.merge import add\n",
    "from keras.layers import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Reshape\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.layers import (LSTM, BatchNormalization, Dense, Dropout, Embedding,\n",
    "                          Input, Lambda, TimeDistributed, GRU, Masking)\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Attention\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import GlobalAveragePooling2D, Permute, Multiply, dot, Dot\n",
    "import random\n",
    "from random import sample\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42c9bf",
   "metadata": {},
   "source": [
    "Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1103265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(alist):\n",
    "    return np.array([pad_sequences([alist], maxlen=40, truncating='post')[0]])\n",
    "\n",
    "def generate_img_feature_vector(filepath, resnet_model):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img.reshape(1, 224, 224, 3)\n",
    "    \n",
    "    feature_vector = resnet_model.predict(img, verbose=0).reshape(1, 2048)\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "def generate_caption(img_feature_vector, model):\n",
    "    caption = [1]\n",
    "    next_word = None\n",
    "#     img_feature_vector = np.array([img_feature_vector])\n",
    "    \n",
    "    while next_word != vocab['eos'] and len(caption) != 40:\n",
    "        output = model.predict([img_feature_vector, pad(caption), np.zeros(shape=(1,512)), np.zeros(shape=(1,512))], verbose=0)\n",
    "        next_word = np.argsort(output)[0][-1]\n",
    "        caption.append(next_word)\n",
    "        \n",
    "    return caption\n",
    "\n",
    "def decode_caption(encoded_caption, vocab):\n",
    "    vocab_inv = vocab_inv = {v: k for k, v in vocab.items()}\n",
    "    decoded_caption = []\n",
    "    for word in encoded_caption:\n",
    "        if word == 0:\n",
    "            continue\n",
    "        decoded_caption.append(vocab_inv[word])\n",
    "    return \" \".join(decoded_caption)\n",
    "\n",
    "def image_caption_generator_greedy(filename, model, vocab):\n",
    "    resnet_model = ResNet50(include_top=True)\n",
    "    resnet_model = Model(inputs=resnet_model.input, outputs=resnet_model.layers[-2].output)\n",
    "    return decode_caption(generate_caption(generate_img_feature_vector(filename, resnet_model), model), vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e4eee3",
   "metadata": {},
   "source": [
    "Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "52b2d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search \n",
    "from math import log, exp\n",
    "\n",
    "def beam_search(fv, model, beam_size, k):\n",
    "    complete_captions = []\n",
    "    captions_tree = [\n",
    "        [([vocab['sos']], 1)]\n",
    "    ]  \n",
    "    for i in range(40):\n",
    "        caps_to_be_expanded = captions_tree[i]\n",
    "        for item in caps_to_be_expanded:\n",
    "            if item[0][-1] == vocab['eos']:\n",
    "                complete_captions.append(item)\n",
    "        caps_to_be_expanded = sorted(filter(lambda t: t[0][-1] != vocab['eos'], caps_to_be_expanded), key=lambda t: t[1])\n",
    "        caps_to_be_expanded = caps_to_be_expanded[-k:]\n",
    "        candidates = []\n",
    "        if len(caps_to_be_expanded) == 0:\n",
    "            return\n",
    "        for caption, prob in caps_to_be_expanded:\n",
    "            output = model.predict([fv, pad(caption), np.zeros(shape=(1,512)), np.zeros(shape=(1,512))], verbose=0)\n",
    "            # output = 2d array\n",
    "            next_words = np.argsort(output)[0][-beam_size:]\n",
    "            for word in next_words:\n",
    "                new_caption = caption + [word]\n",
    "                new_prob = (log(output[0][word])+prob)*(1/len(new_caption)**0)\n",
    "                candidates.append((new_caption, new_prob))\n",
    "        captions_tree.append(candidates)\n",
    "    return complete_captions\n",
    "\n",
    "def image_caption_generator_beam(filename, model, beam_size, k, vocab):\n",
    "    resnet_model = ResNet50(include_top=True)\n",
    "    resnet_model = Model(inputs=resnet_model.input, outputs=resnet_model.layers[-2].output)\n",
    "    fv = generate_img_feature_vector(filename, resnet_model)\n",
    "#     fv = np.array([fv])\n",
    "    \n",
    "    complete_captions = beam_search(fv, model, beam_size, k)\n",
    "    \n",
    "    sorted_list = sorted(complete_captions, key=lambda x: x[1])\n",
    "    \n",
    "    return decode_caption(sorted_list[-1][0], vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d334a12",
   "metadata": {},
   "source": [
    "Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ebab5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucleus_sampling(softmax, p, temp):\n",
    "    actual_p = 0\n",
    "    x = np.argsort(softmax)\n",
    "    for item in x[::-1]:\n",
    "        if actual_p < p:\n",
    "            actual_p = actual_p+softmax[item]\n",
    "        else:\n",
    "            softmax[item] = 0\n",
    "    softmax = normalize([softmax], norm='l1')[0]\n",
    "    softmax = apply_temp(softmax, temp)\n",
    "    next_word = random.choices(list(range(0, len(softmax))), weights = softmax, k=1)[0]\n",
    "    \n",
    "    return next_word\n",
    "\n",
    "def apply_temp(softmax, temperature):\n",
    "    softmax = np.log(softmax) / temperature\n",
    "    softmax = np.exp(softmax)\n",
    "    softmax = softmax / np.sum(softmax)\n",
    "    return softmax\n",
    "\n",
    "def generate_caption_nucleus_sampling(img_feature_vector, model):\n",
    "    caption = [1]\n",
    "    next_word = None\n",
    "\n",
    "    while next_word != vocab['eos'] and len(caption) != 40:\n",
    "        output = model.predict([img_feature_vector, pad(caption), np.zeros(shape=(1,512)), np.zeros(shape=(1,512))], verbose=0)\n",
    "\n",
    "        next_word = nucleus_sampling(output[0], 0.9, 0.5)\n",
    "\n",
    "        caption.append(next_word)\n",
    "\n",
    "    return caption\n",
    "\n",
    "def image_caption_generator_nucleus_sampling(filename, model, vocab):\n",
    "    resnet_model = ResNet50(include_top=True)\n",
    "    resnet_model = Model(inputs=resnet_model.input, outputs=resnet_model.layers[-2].output)\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    fv = generate_img_feature_vector(filename, resnet_model)\n",
    "#     fv = np.array([fv])\n",
    "    return decode_caption(generate_caption_nucleus_sampling(fv, model), vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5821b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"main\":\n",
    "    filename = \"Flickr_Data/Images/1001773457_577c3a7d70.jpg\"\n",
    "    model = create_model()\n",
    "    model.load_weights(\"30k_final_eval/show_and_tell_1_without_reg/final_model.h5\")\n",
    "    vocab = np.load(\"vocab.npy\", allow_pickle=True).item()\n",
    "    print(\"Nucleus Sampling: \"+image_caption_generator_nucleus_sampling(filename, model))\n",
    "    print(\"Greedy: \"+image_caption_generator_greedy(filename, model))\n",
    "    print(\"Beam: \"+image_caption_generator_beam(filename, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00d2d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def create_model():\n",
    "    vocab_size = 5185+1\n",
    "    max_length = 40\n",
    "    unit_size = 512\n",
    "\n",
    "    # image feature extractor model\n",
    "    inputs1 = Input(shape=(2048,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(512, activation='relu')(fe1)\n",
    "    fe3 = BatchNormalization()(fe2)\n",
    "    fe4 = Lambda(lambda x : K.expand_dims(x, axis=1))(fe3)\n",
    "\n",
    "    # partial caption sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 512, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)  \n",
    "\n",
    "    LSTMLayer = LSTM(512, return_state = True, dropout=0.5)\n",
    "\n",
    "    a0 = Input(shape=(unit_size,))\n",
    "    c0 = Input(shape=(unit_size,))\n",
    "\n",
    "    a, b, c = LSTMLayer(fe4, initial_state = [a0, c0])\n",
    "\n",
    "    A,_,_ = LSTMLayer(se2, initial_state=[b,c])\n",
    "\n",
    "    outputs = Dense(vocab_size, activation='softmax')(A)\n",
    "\n",
    "    # merge the two input models\n",
    "    model = Model(inputs=[inputs1, inputs2, a0, c0], outputs=outputs)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
