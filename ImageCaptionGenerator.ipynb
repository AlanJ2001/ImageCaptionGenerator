{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30965abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "179934b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = 'Flickr_Data/Images/'\n",
    "images = glob(images_path+'*.jpg')\n",
    "captions_path = \"Flickr_Data/Flickr_TextData/FLickr8k.token.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe7e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResNet50(include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e224b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure model\n",
    "resnet_model = Model(inputs=resnet_model.input, outputs=resnet_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f12d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the images\n",
    "img_feature_vectors = {}\n",
    "\n",
    "count = 0\n",
    "for item in images:\n",
    "    img = cv2.imread(item)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img.reshape(1, 224, 224, 3)\n",
    "    \n",
    "    feature_vector = resnet_model.predict(img, verbose=0).reshape(2048,)\n",
    "    \n",
    "    img_filename = item.split('\\\\')[-1]\n",
    "    img_feature_vectors[img_filename] = feature_vector\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if (count%50==0):\n",
    "        print(count)\n",
    "    \n",
    "    if (count==1499):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632601f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('image_feature_vectors.npy', img_feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c3c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in dictionary of image feature vectors\n",
    "img_feature_vectors = np.load('image_feature_vectors.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77452d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes a string lowercase, prepends it with the string 'sos' and appends with 'eos'\n",
    "def process_string(s):\n",
    "    s = s.lower()\n",
    "    return 'sos ' + s + ' eos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a291a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the captions data\n",
    "captions_dict = {}\n",
    "\n",
    "f = open(captions_path, 'r').read().split('\\n')\n",
    "\n",
    "# generate a dictionary of filenames to a list of captions\n",
    "for line in f:\n",
    "    try:\n",
    "        filename_caption = line.split('\\t') \n",
    "        filename = filename_caption[0][:-2]\n",
    "        caption = process_string(filename_caption[1])\n",
    "        \n",
    "        if filename in img_feature_vectors:\n",
    "            if filename not in captions_dict:\n",
    "                captions_dict[filename] = [caption]\n",
    "            else:\n",
    "                captions_dict[filename].append(caption)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc62fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dictionary called vocab which will contain all words in the set of captions mapped to a unique integer\n",
    "vocab = {}\n",
    "\n",
    "count = 1\n",
    "for filename, captions in captions_dict.items():\n",
    "    for caption in captions:\n",
    "        caption_as_list = caption.split()\n",
    "        for word in caption_as_list:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = count\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b7ff4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a string and returns a list of integers where each integer corresponds to a particular word\n",
    "def encode_string(s, adict):\n",
    "    s_list = s.split()\n",
    "    encoded_string = []\n",
    "    for word in s_list:\n",
    "        encoded_string.append(adict[word])\n",
    "    return encoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61624e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a copy of captions_dict where each caption is replaced with a list of integers where each integer corresponds to a word in the caption\n",
    "captions_dict_encoded = copy.deepcopy(captions_dict)\n",
    "\n",
    "for filename, captions in captions_dict_encoded.items():\n",
    "    for i, caption in enumerate(captions):\n",
    "        captions[i] = encode_string(caption, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5249b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get length of the longest caption in the data set\n",
    "max_len = 0\n",
    "for filename, captions in captions_dict_encoded.items():\n",
    "    for caption in captions:\n",
    "        if len(caption)>max_len:\n",
    "            max_len = len(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "072183e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict_encoded = np.load('captions_dict_encoded.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5e35a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(image_feature_vectors, captions_dict_encoded):\n",
    "    X = []\n",
    "    y_in = []\n",
    "    y_out = []\n",
    "    \n",
    "    for filename, captions in captions_dict_encoded.items():\n",
    "        for caption in captions:\n",
    "            i = 0\n",
    "            for word in caption:\n",
    "                y_in_item = caption[:i]\n",
    "                y_in_item = (y_in_item + max_len * [0])[:max_len]\n",
    "                y_in.append(y_in_item)\n",
    "                \n",
    "                y_out_item = [0]*len(vocab)\n",
    "                y_out_item[word-1] = 1\n",
    "                y_out.append(y_out_item)\n",
    "                \n",
    "                X_item = image_feature_vectors[filename]\n",
    "                X.append(X_item)\n",
    "                i+=1\n",
    "    return X, y_in, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "325a0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_in, y_out = generate_training_data(img_feature_vectors, captions_dict_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "279890da",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del y_in\n",
    "del y_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
