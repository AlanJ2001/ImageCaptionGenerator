{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30965abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "179934b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = 'Flickr_Data/Images/'\n",
    "images = glob(images_path+'*.jpg')\n",
    "captions_path = \"Flickr_Data/Flickr_TextData/FLickr8k.token.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe7e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResNet50(include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e224b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure model\n",
    "resnet_model = Model(inputs=resnet_model.input, outputs=resnet_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f12d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the images\n",
    "img_feature_vectors = {}\n",
    "\n",
    "count = 0\n",
    "for item in images:\n",
    "    img = cv2.imread(item)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img.reshape(1, 224, 224, 3)\n",
    "    \n",
    "    feature_vector = resnet_model.predict(img, verbose=0).reshape(2048,)\n",
    "    \n",
    "    img_filename = item.split('\\\\')[-1]\n",
    "    img_feature_vectors[img_filename] = feature_vector\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if (count%50==0):\n",
    "        print(count)\n",
    "    \n",
    "    if (count==1499):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632601f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('image_feature_vectors.npy', img_feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c3c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in dictionary of image feature vectors\n",
    "img_feature_vectors = np.load('image_feature_vectors.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77452d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes a string lowercase, prepends it with the string 'sos' and appends with 'eos'\n",
    "def process_string(s):\n",
    "    s = s.lower()\n",
    "    return 'sos ' + s + ' eos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a291a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the captions data\n",
    "captions_dict = {}\n",
    "\n",
    "f = open(captions_path, 'r').read().split('\\n')\n",
    "\n",
    "# generate a dictionary of filenames to a list of captions\n",
    "for line in f:\n",
    "    try:\n",
    "        filename_caption = line.split('\\t') \n",
    "        filename = filename_caption[0][:-2]\n",
    "        caption = process_string(filename_caption[1])\n",
    "        \n",
    "        if filename in img_feature_vectors:\n",
    "            if filename not in captions_dict:\n",
    "                captions_dict[filename] = [caption]\n",
    "            else:\n",
    "                captions_dict[filename].append(caption)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc62fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dictionary called vocab which will contain all words in the set of captions mapped to a unique integer\n",
    "vocab = {}\n",
    "\n",
    "count = 1\n",
    "for filename, captions in captions_dict.items():\n",
    "    for caption in captions:\n",
    "        caption_as_list = caption.split()\n",
    "        for word in caption_as_list:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = count\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b7ff4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a string and returns a list of integers where each integer corresponds to a particular word\n",
    "def encode_string(s, adict):\n",
    "    s_list = s.split()\n",
    "    encoded_string = []\n",
    "    for word in s_list:\n",
    "        encoded_string.append(adict[word])\n",
    "    return encoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61624e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a copy of captions_dict where each caption is replaced with a list of integers where each integer corresponds to a word in the caption\n",
    "captions_dict_encoded = copy.deepcopy(captions_dict)\n",
    "\n",
    "for filename, captions in captions_dict_encoded.items():\n",
    "    for i, caption in enumerate(captions):\n",
    "        captions[i] = encode_string(caption, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5249b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get length of the longest caption in the data set\n",
    "max_len = 0\n",
    "for filename, captions in captions_dict_encoded.items():\n",
    "    for caption in captions:\n",
    "        if len(caption)>max_len:\n",
    "            max_len = len(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "072183e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict_encoded = np.load('captions_dict_encoded.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5e35a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(image_feature_vectors, captions_dict_encoded):\n",
    "    X = []\n",
    "    y_in = []\n",
    "    y_out = []\n",
    "    \n",
    "    for filename, captions in captions_dict_encoded.items():\n",
    "        for caption in captions:\n",
    "            i = 0\n",
    "            for word in caption:\n",
    "                y_in_item = caption[:i]\n",
    "                y_in_item = (y_in_item + max_len * [0])[:max_len]\n",
    "                y_in.append(y_in_item)\n",
    "                \n",
    "                y_out_item = [0]*len(vocab)\n",
    "                y_out_item[word-1] = 1\n",
    "                y_out.append(y_out_item)\n",
    "                \n",
    "                X_item = image_feature_vectors[filename]\n",
    "                X.append(X_item)\n",
    "                i+=1\n",
    "    return X, y_in, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "325a0e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.333067417144775\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "X, y_in, y_out = generate_training_data(img_feature_vectors, captions_dict_encoded)\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9406a394",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19516/2720866710.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float64\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "y_in = np.array(y_in, dtype='float64')\n",
    "y_out = np.array(y_out, dtype=\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c07003bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X.npy', X)\n",
    "np.save('y_in.npy', y_in)\n",
    "np.save('y_out.npy', y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "279890da",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del y_in\n",
    "del y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c7a1642",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X.npy', allow_pickle='TRUE')\n",
    "y_in = np.load('y_in.npy', allow_pickle='TRUE')\n",
    "y_out = np.load('y_out.npy', allow_pickle='TRUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0d13ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "#image\n",
    "x = keras.Sequential([\n",
    "    keras.layers.Input(shape=(2048,)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.RepeatVector(max_len)\n",
    "])\n",
    "\n",
    "#caption\n",
    "y = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_len, embedding_size, input_length=max_len),\n",
    "    keras.layers.LSTM(embedding_size, return_sequences=True)\n",
    "])\n",
    "\n",
    "#concatinate outputs of y and x\n",
    "z = keras.layers.Concatenate()([x.output, y.output])\n",
    "z = keras.layers.LSTM(128, return_sequences=True)(z)\n",
    "z = keras.layers.LSTM(512, return_sequences=False)(z)\n",
    "z = keras.layers.Dense(vocab_len, activation='softmax')(z)\n",
    "\n",
    "model = Model(inputs=[x.input, y.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a81042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
