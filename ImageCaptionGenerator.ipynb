{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30965abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "import sys\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179934b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = 'Flickr_Data/Images/'\n",
    "images = glob(images_path+'*.jpg')\n",
    "captions_path = \"Flickr_Data/Flickr_TextData/FLickr8k.token.txt\"\n",
    "max_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe7e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResNet50(include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e224b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure model\n",
    "resnet_model = Model(inputs=resnet_model.input, outputs=resnet_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f12d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the images\n",
    "# generate a dictionary of image filename -> feature vector\n",
    "\n",
    "# start_index is inclusive\n",
    "\n",
    "def generate_feature_vectors(start_index, num_of_images, images, model):\n",
    "    img_feature_vectors = {}\n",
    "\n",
    "    count = 0\n",
    "    for item in images[start_index:]:\n",
    "        img = cv2.imread(item)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = img.reshape(1, 224, 224, 3)\n",
    "\n",
    "        feature_vector = model.predict(img, verbose=0).reshape(2048,)\n",
    "\n",
    "        img_filename = item.split('\\\\')[-1]\n",
    "        img_feature_vectors[img_filename] = feature_vector\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if (count%50==0):\n",
    "            print(count)\n",
    "\n",
    "        if (count==num_of_images):\n",
    "            break\n",
    "\n",
    "    return img_feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77452d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes a string lowercase, prepends it with the string 'sos' and appends with 'eos'\n",
    "def process_string(s):\n",
    "    s = s.lower()\n",
    "    return 'sos ' + s + ' eos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the captions data\n",
    "\n",
    "# generate a dictionary of image filename -> list of captions\n",
    "\n",
    "def generate_captions_dict(captions_path, img_feature_vectors):\n",
    "    captions_dict = {}\n",
    "\n",
    "    f = open(captions_path, 'r').read().split('\\n')\n",
    "\n",
    "    # generate a dictionary of filenames to a list of captions\n",
    "    for line in f:\n",
    "        try:\n",
    "            filename_caption = line.split('\\t') \n",
    "            filename = filename_caption[0][:-2]\n",
    "            caption = process_string(filename_caption[1])\n",
    "\n",
    "            if filename in img_feature_vectors:\n",
    "                if filename not in captions_dict:\n",
    "                    captions_dict[filename] = [caption]\n",
    "                else:\n",
    "                    captions_dict[filename].append(caption)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return captions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a list of all captions in a text file \n",
    "\n",
    "def get_list_of_captions(captions_path):\n",
    "    f = open(captions_path, 'r').read().split('\\n')\n",
    "    all_captions = []\n",
    "\n",
    "    for line in f:\n",
    "        try:\n",
    "            filename_caption = line.split('\\t') \n",
    "            caption = process_string(filename_caption[1])\n",
    "            all_captions.append(caption)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return all_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc62fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dictionary called vocab which will contain all words in the list of captions mapped to a unique integer\n",
    "\n",
    "# def create_full_vocab(captions_path):\n",
    "#     vocab = {}\n",
    "#     all_captions = get_list_of_captions(captions_path)\n",
    "#     count = 1\n",
    "#     for caption in all_captions:\n",
    "#         caption_as_list = caption.split()\n",
    "#         for word in caption_as_list:\n",
    "#             if word not in vocab:\n",
    "#                 vocab[word] = count\n",
    "#                 count += 1\n",
    "#     return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca915d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(captions_dict):\n",
    "    vocab = {}\n",
    "    count = 1\n",
    "    for filename, captions in captions_dict.items():\n",
    "        for caption in captions:\n",
    "            caption_as_list = caption.split()\n",
    "            for word in caption_as_list:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = count\n",
    "                    count += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate a dictionary called vocab which will contain all words in the list of captions mapped to a unique integer\n",
    "# # does not include words that appear less than 10 times\n",
    "# only keeps common words\n",
    "\n",
    "def create_common_vocab(captions_path):\n",
    "    vocab_freq = {}\n",
    "    vocab_dict = {}\n",
    "    all_captions = get_list_of_captions(captions_path)\n",
    "    count = 1\n",
    "    for caption in all_captions:\n",
    "        caption_as_list = caption.split()\n",
    "        for word in caption_as_list:\n",
    "            if word not in vocab_freq:\n",
    "                vocab_freq[word] = 1\n",
    "            else:\n",
    "                vocab_freq[word] = vocab_freq[word]+1\n",
    "    vocab_list = [w for w in vocab_freq if vocab_freq[w] >= 10]\n",
    "    count = 1\n",
    "    for word in vocab_list:\n",
    "        if word not in vocab_dict:\n",
    "            vocab_dict[word] = count\n",
    "            count+=1\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ff4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a string and returns a list of integers where each integer corresponds to a particular word\n",
    "def encode_string(s, vocab):\n",
    "    s_list = s.split()\n",
    "    encoded_string = []\n",
    "    for word in s_list:\n",
    "            if word in vocab:\n",
    "                encoded_string.append(vocab[word])\n",
    "    return encoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61624e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a copy of captions_dict where each caption is replaced with a list of integers where each integer corresponds to a word in the caption\n",
    "\n",
    "def encode_captions_dict(captions_dict, vocab):\n",
    "    captions_dict_encoded = copy.deepcopy(captions_dict)\n",
    "\n",
    "    for filename, captions in captions_dict_encoded.items():\n",
    "        for i, caption in enumerate(captions):\n",
    "            captions[i] = encode_string(caption, vocab)\n",
    "    return captions_dict_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5249b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get length of the longest caption in the data set\n",
    "# max_len = 0\n",
    "# for caption in all_captions:\n",
    "#     if len(caption.split())>max_len:\n",
    "#         max_len = len(caption.split())\n",
    "# max_len # = 40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d82825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from npy_append_array import NpyAppendArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e35a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_training_data(image_feature_vectors, captions_dict_encoded, max_len, vocab_len):\n",
    "#     X = []\n",
    "#     y_in = []\n",
    "#     y_out = []\n",
    "    \n",
    "#     for filename, captions in captions_dict_encoded.items():\n",
    "#         for caption in captions:\n",
    "#             i = 0\n",
    "#             for word in caption:\n",
    "#                 y_in_item = caption[:i]\n",
    "#                 y_in_item = (y_in_item + max_len * [0])[:max_len]\n",
    "#                 y_in.append(y_in_item)\n",
    "                \n",
    "#                 y_out_item = [0]*(vocab_len+1)\n",
    "#                 y_out_item[word] = 1\n",
    "#                 y_out.append(y_out_item)\n",
    "                \n",
    "#                 X_item = image_feature_vectors[filename]\n",
    "#                 X.append(X_item)\n",
    "#                 i+=1\n",
    "#     return X, y_in, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_training_data(image_feature_vectors, captions_dict_encoded, max_len, vocab_len):\n",
    "#     X = 'X.npy'\n",
    "#     y_in = 'y_in.npy'\n",
    "#     y_out = 'y_out.npy'\n",
    "#     count = 0\n",
    "    \n",
    "#     for filename, captions in captions_dict_encoded.items():\n",
    "#         print(count)\n",
    "#         for caption in captions:\n",
    "#             i = 0\n",
    "#             for word in caption:\n",
    "#                 y_in_item = caption[:i]\n",
    "#                 y_in_item = (y_in_item + max_len * [0])[:max_len]\n",
    "#                 with NpyAppendArray(y_in) as npaa:\n",
    "#                     npaa.append(np.array(y_in_item))\n",
    "                \n",
    "#                 y_out_item = [0]*(vocab_len+1)\n",
    "#                 y_out_item[word] = 1\n",
    "#                 with NpyAppendArray(y_out) as npaa:\n",
    "#                     npaa.append(np.array(y_out_item))\n",
    "                \n",
    "#                 X_item = image_feature_vectors[filename]\n",
    "#                 with NpyAppendArray(X) as npaa:\n",
    "#                     npaa.append(np.array(X_item))\n",
    "#                 i+=1\n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_generator(image_feature_vectors, captions_dict_encoded, batch_size):\n",
    "#     X = []\n",
    "#     y_in = []\n",
    "#     y_out = []\n",
    "#     n = 0\n",
    "#     while 1:\n",
    "#         for filename, captions in captions_dict_encoded.items():\n",
    "#             n+=1\n",
    "#             for caption in captions:\n",
    "#                 i = 0\n",
    "#                 for word in caption:\n",
    "#                     y_in_item = caption[:i]\n",
    "#                     y_in_item = (y_in_item + max_len * [0])[:max_len]\n",
    "#                     y_in.append(y_in_item)\n",
    "\n",
    "#                     y_out_item = [0]*(3988+1)\n",
    "#                     y_out_item[word] = 1\n",
    "#                     y_out.append(y_out_item)\n",
    "\n",
    "#                     X_item = image_feature_vectors[filename]\n",
    "#                     X.append(X_item)\n",
    "#                     i+=1\n",
    "#             if n==batch_size:\n",
    "#                 yield [[np.array(X), np.array(y_in)], np.array(y_out)]\n",
    "#                 X = []\n",
    "#                 y_in = []\n",
    "#                 y_out = []\n",
    "#                 n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7689a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(image_feature_vectors, captions_dict_encoded, max_len, vocab_len, batch_size):\n",
    "    X_file = 'X.npy'\n",
    "    y_in_file = 'y_in.npy'\n",
    "    y_out_file = 'y_out.npy'\n",
    "    X = []\n",
    "    y_in = []\n",
    "    y_out = []\n",
    "    n = 0\n",
    "    for filename, captions in captions_dict_encoded.items():\n",
    "        n+=1\n",
    "        for caption in captions:\n",
    "            i = 0\n",
    "            for word in caption:\n",
    "                y_in_item = caption[:i]\n",
    "                y_in_item = (y_in_item + max_len * [0])[:max_len]\n",
    "                y_in.append(y_in_item)\n",
    "\n",
    "                y_out_item = [0]*(vocab_len+1)\n",
    "                y_out_item[word] = 1\n",
    "                y_out.append(y_out_item)\n",
    "\n",
    "                X_item = image_feature_vectors[filename]\n",
    "                X.append(X_item)\n",
    "                i+=1\n",
    "        if n==batch_size:\n",
    "            print(n)\n",
    "            with NpyAppendArray(X_file) as npaa:\n",
    "                npaa.append(np.array(X))\n",
    "            with NpyAppendArray(y_in_file) as npaa:\n",
    "                npaa.append(np.array(y_in))\n",
    "            with NpyAppendArray(y_out_file) as npaa:\n",
    "                npaa.append(np.array(y_out))\n",
    "            X = []\n",
    "            y_in = []\n",
    "            y_out = []\n",
    "            n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feature_vectors = generate_feature_vectors(0, 8091, images, resnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b98763",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict = generate_captions_dict(captions_path, img_feature_vectors)\n",
    "vocab = create_common_vocab(captions_path)\n",
    "captions_dict_encoded = encode_captions_dict(captions_dict, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab16b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0888e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('img_feature_vectors.npy', img_feature_vectors)\n",
    "# np.save('vocab.npy', vocab)\n",
    "# np.save('captions_dict_encoded.npy', captions_dict_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d486c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feature_vectors = np.load('img_feature_vectors.npy', allow_pickle='TRUE').item()\n",
    "captions_dict_encoded = np.load('captions_dict_encoded.npy', allow_pickle='TRUE').item()\n",
    "vocab = np.load('vocab.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "generate_training_data(img_feature_vectors, captions_dict_encoded, 40, vocab_len, 1000)\n",
    "\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07003bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('X.npy', X)\n",
    "# np.save('y_in.npy', y_in)\n",
    "# np.save('y_out.npy', y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279890da",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del y_in\n",
    "del y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf973eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next stage - model creation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d13ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 1\n",
    "\n",
    "embedding_size = 128\n",
    "vocab_len = 3988+1\n",
    "max_len = 40\n",
    "\n",
    "#image\n",
    "x = keras.Sequential([\n",
    "    keras.layers.Input(shape=(2048,)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.RepeatVector(max_len)\n",
    "])\n",
    "\n",
    "#caption\n",
    "y = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_len, embedding_size, input_length=max_len),\n",
    "    keras.layers.LSTM(256, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(embedding_size))\n",
    "])\n",
    "\n",
    "#concatinate outputs of y and x\n",
    "z = keras.layers.Concatenate()([x.output, y.output])\n",
    "z = keras.layers.LSTM(128, return_sequences=True)(z)\n",
    "z = keras.layers.LSTM(512, return_sequences=False)(z)\n",
    "z = keras.layers.Dense(vocab_len, activation='softmax')(z)\n",
    "\n",
    "model = Model(inputs=[x.input, y.input], outputs=z)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea2a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 2\n",
    "\n",
    "vocab_size = 1965+1\n",
    "max_len = 40\n",
    "embedding_size = 200\n",
    "\n",
    "# image feature extractor model\n",
    "image_input = Input(shape=(2048,))\n",
    "ii1 = Dropout(0.5)(image_input)\n",
    "ii2 = Dense(256, activation='relu')(ii1)\n",
    "\n",
    "# partial caption sequence model\n",
    "caption_input = Input(shape=(max_len,))\n",
    "ci1 = Embedding(vocab_size, embedding_size, mask_zero=True)(caption_input)\n",
    "ci2 = Dropout(0.5)(ci1)\n",
    "ci3 = LSTM(256)(ci2)\n",
    "\n",
    "# decoder (feed forward) model\n",
    "output = add([ii2, ci3])\n",
    "output = Dense(256, activation='relu')(output)\n",
    "output = Dense(vocab_size, activation='softmax')(output)\n",
    "\n",
    "# merge the two input models\n",
    "model = Model(inputs=[image_input, caption_input ], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be3d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64befb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('models/image_caption_gen_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12934407",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inv = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(img_feature_vector):\n",
    "    encoded_caption = np.zeros(max_len)\n",
    "    encoded_caption[0] = 1\n",
    "    encoded_caption = np.array([encoded_caption])\n",
    "    \n",
    "    for i in range(max_len-1):\n",
    "        next_word = np.argmax(model.predict([img_feature_vector, encoded_caption]))\n",
    "        encoded_caption[0][i+1] = next_word\n",
    "        if next_word == vocab['eos']:\n",
    "            break;\n",
    "    return encoded_caption[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22904c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img_feature_vector(filename):\n",
    "    img = cv2.imread(filename)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img.reshape(1, 224, 224, 3)\n",
    "    \n",
    "    feature_vector = resnet_model.predict(img, verbose=0).reshape(1, 2048)\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4dc806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_caption(encoded_caption):\n",
    "    decoded_caption = []\n",
    "    for word in encoded_caption:\n",
    "        if word == 0:\n",
    "            return \" \".join(decoded_caption)\n",
    "        decoded_caption.append(vocab_inv[word])\n",
    "    return \" \".join(decoded_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa35abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_num = 8011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee7092",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('models/image_caption_gen_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('models/image_caption_gen_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_caption(generate_caption(generate_img_feature_vector(images[img_num])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ef689",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "img = cv2.imread(images[img_num])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d657d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaccb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "f = open(\"glove/glove.6B.200d.txt\", encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "    vector = np.asarray(word_vector[1:], dtype='float32')\n",
    "    word_embeddings[word] = vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d34f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "vocab_size = 1965+1\n",
    "# word to index\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "\n",
    "for word, i in vocab.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = word_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embedding_matrix.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.load('X.npy', allow_pickle='TRUE')\n",
    "# y_in = np.load('y_in.npy', allow_pickle='TRUE')\n",
    "# y_out = np.load('y_out.npy', allow_pickle='TRUE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
